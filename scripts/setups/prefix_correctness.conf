# BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128]
BATCH_SIZES = [4]
# order:  num_query_heads, num_kv_heads
NUM_HEADS = [[32, 8]]

# SEQUENCE_LENGTHS = [16, 32, 64, 128, 512, 1024, 2048, 4096]
# SEQUENCE_LENGTHS = [64]
SEQUENCE_LENGTHS = [1024]
# PREFIX_PREFILL_SHARE_OF_DECODE = [0.0, 0.5, 1.0]
PREFIX_PREFILL_SHARE_OF_DECODE = [0.5]
# PREFIX_PREFILL_SHARE_OF_PARTIAL_PREFILL = [0.0, 0.5]
PREFIX_PREFILL_SHARE_OF_PARTIAL_PREFILL = [0.5]
PREFIX_PREFILL_BATCH_COMPOSITION = ["ALTERNATING"]

HEAD_SIZES = [128]  # only powers of 2! for llama2 & 3
# head_size * head_numbers = hidden_size

BLOCK_SIZES = [16]
NUM_BLOCKS = [4321]  # "arbitrary values for testing..."

# PROMPT_PATTERNS = [[1.0], [0.1, 0.4, 0.5, 1.0, 0.2]]
PROMPT_PATTERNS = [[1.0]]

MAX_VALUES = [1.0]
BENCHMARK_MODES = ["CUDA_EVENTS"]

# IMPLEMENTATION_UT = ["FLASH_ATTN", "UNF_TRITON_3D", "UNF_TRITON_2D"]
# IMPLEMENTATION_UT = ["UNF_TRITON_3D", "UNF_TRITON_2D", "UNF_TRITON_AUTO"]
# IMPLEMENTATION_UT = ["UNF_TRITON_2D"]
IMPLEMENTATION_UT = ["FLASH_ATTN", "UNF_TRITON_2D"]

TRITON_BACKEND_DEBUG = 1
