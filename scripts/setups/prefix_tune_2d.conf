BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128]
# BATCH_SIZES = [4]
# order:  num_query_heads, num_kv_heads
NUM_HEADS = [[32, 8]]

SEQUENCE_LENGTHS = [16, 32, 64, 128, 512, 1024, 2048, 4096]
# SEQUENCE_LENGTHS = [64]
PREFIX_PREFILL_SHARE_OF_DECODE = [0.0, 0.5, 1.0]
# PREFIX_PREFILL_SHARE_OF_DECODE = [0.0, 0.5]
# PREFIX_PREFILL_SHARE_OF_DECODE = [0.5]
PREFIX_PREFILL_SHARE_OF_PARTIAL_PREFILL = [0.0, 0.5]
# PREFIX_PREFILL_SHARE_OF_PARTIAL_PREFILL = [0.5]
# PREFIX_PREFILL_BATCH_COMPOSITION = ["ALTERNATING"]
# PREFIX_PREFILL_BATCH_COMPOSITION = ["DEC_PRE"]
PREFIX_PREFILL_BATCH_COMPOSITION = ["DEC_PRE", "ALTERNATING"]

HEAD_SIZES = [128]  # only powers of 2! for llama2 & 3
# head_size * head_numbers = hidden_size

BLOCK_SIZES = [16]
NUM_BLOCKS = [4321]  # "arbitrary values for testing..."

PROMPT_PATTERNS = [[1.0], [0.1, 0.4, 0.5, 1.0, 0.2]]
# PROMPT_PATTERNS = [[1.0]]

MAX_VALUES = [1.0]
# BENCHMARK_MODES = ["CUDA_EVENTS"]
BENCHMARK_MODES = ["CUDA_GRAPHS"]

# IMPLEMENTATION_UT = ["UNF_TRITON_2D"]
# IMPLEMENTATION_UT = ["UNF_TRITON_2D_SIMPLE"]
# IMPLEMENTATION_UT = ["FLASH_ATTN", "UNF_TRITON_2D"]
# IMPLEMENTATION_UT = ["NT_UNF_TRITON_2D", "NT_UNF_TRITON_3D", "FLASH_ATTN", "UNF_TRITON_2D", "UNF_TRITON_3D"]
IMPLEMENTATION_UT = ["NT_UNF_TRITON_2D", "NT_UNF_TRITON_3D", "UNF_TRITON_2D", "UNF_TRITON_3D"]
# IMPLEMENTATION_UT = ["UNF_TRITON_3D"]

# TRITON_BACKEND_DEBUG = 1
# STORE_TEST_RESULT_PATH=/results
STORE_TEST_RESULT_PATH=./zrl-triton-results-and-notebooks/micro_benchmarks/raw_data/

# TEST_ALLOW_INCORRECT = 1
