BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128]
# BATCH_SIZES = [16]
# order:  num_query_heads, num_kv_heads
NUM_HEADS = [[32, 8]]

SEQUENCE_LENGTHS = [16, 32, 64, 128, 512, 1024, 2048, 4096]
# SEQUENCE_LENGTHS = [4096]

HEAD_SIZES = [128]  # only powers of 2! for llama2 & 3
# head_size * head_numbers = hidden_size

BLOCK_SIZES = [16]
NUM_BLOCKS = [4321]  # "arbitrary values for testing..."

MAX_VALUES = [1.0]
BENCHMARK_MODES = ["CUDA_EVENTS"]
# BENCHMARK_MODES = ["CUDA_GRAPHS"]

# IMPLEMENTATION_UT = ["TRITON_RESHAPE_AND_CACHE", "VLLM_CUDA_RESHAPE_AND_CACHE"]
# IMPLEMENTATION_UT = ["VLLM_CUDA_RESHAPE_AND_CACHE"]
IMPLEMENTATION_UT = ["TRITON_RESHAPE_AND_CACHE"]

# TRITON_BACKEND_DEBUG = 1
# STORE_TEST_RESULT_PATH=/results
STORE_TEST_RESULT_PATH=./zrl-triton-results-and-notebooks/micro_benchmarks/raw_data/

# TEST_ALLOW_INCORRECT = 1
