DTYPES = ["float16"]
SEEDS = [0]
BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128]
# order:  num_query_heads, num_kv_heads
NUM_HEADS = [[32, 8]]

SEQUENCE_LENGTHS = [16, 32, 64, 128, 512, 1024, 2048, 4096]
PREFIX_PREFILL_SHARE_OF_DECODE = [0.0, 0.5, 1.0]
PREFIX_PREFILL_SHARE_OF_PARTIAL_PREFILL = [0.0, 0.5]
PREFIX_PREFILL_BATCH_COMPOSITION = ["ALTERNATING"]

HEAD_SIZES = [128]  # only powers of 2! for llama2 & 3
# head_size * head_numbers = hidden_size

BLOCK_SIZES = [16]
NUM_BLOCKS = [4321]  # "arbitrary values for testing..."

PROMPT_PATTERNS = [[1.0], [0.1, 0.4, 0.5, 1.0, 0.2]]

MAX_VALUES = [1.0]
BENCHMARK_MODES = ["CUDA_EVENTS"]

IMPLEMENTATION_UT = ["FLASH_ATTN", "UNF_TRITON_3D", "UNF_TRITON_2D"]
